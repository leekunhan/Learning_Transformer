# Transformer Learning

<p align="center">
<img height="400" src="./image.png" >  
</p>
<p align="center">
Picture：Transformer architecture
</p>

The `Transformer architecture` might be the most significant invention in the field of computer science (specifically natural language processing) in recent years, becoming the backbone of various cutting-edge NLP applications. With its parallel processing capabilities, it allows for more efficient and scalable models, making it easier to train them on large datasets. It has also demonstrated superior performance in several NLP tasks, such as sentiment analysis and text generation.
## Keywords You Need To Understand
> - Encoder
> - Decoder
> - Self Attention / Attention
> - Multi-Head Attention
> - Feed-Forward Network
> - Layer Normalization
> - Tokenizer

## ToDo
In these two weeks, you have two tasks to complete:
1. Fully understand the transformer architecture by reading the paper below and making a report to your mentor. You can refer to the sources below to gain a deeper understanding.
- [ ] Read the paper and make report - [attention is all you need](https://arxiv.org/abs/1706.03762)
  - [ ] put all reference you look, at the end of report page.
2. Complete the practice part by coding to familiarize yourself with how attention works.
- [ ] Finish dot_product_qkv.ipynb file list in [pratice](./Practice/README.md) section.

### Reference Source
> NOTE : The order below does not indicate the importance   

- Medium - [Drawing the Transformer Network from Scratch](https://towardsdatascience.com/drawing-the-transformer-network-from-scratch-part-1-9269ed9a2c5e)  
- Deepgram - [Visualizing and Explaining Transformer Models From the Ground Up](https://deepgram.com/learn/visualizing-and-explaining-transformer-models-from-the-ground-up)
- Youtube - [Word2Vec](https://www.youtube.com/watch?v=j9YNHnCRkig)
- Youtube - [機器學習2021 影片10 ~ 13 ](https://youtube.com/playlist?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&si=Wa9VQkEq22ttKPyJ)by Hung-yi Lee
- Youtube - [Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.](https://www.youtube.com/watch?v=ISNdQcPhsts&t=9595s)
- Youtube - [Attention in transformers, visually explained](https://www.youtube.com/watch?v=eMlx5fFNoYc)
- Arxiv - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
- Arxiv - [Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent Neural Networks](https://arxiv.org/abs/1909.09586)

### Some Realtive Papers & Docs (Optional)
- Arxiv - [Introduction to Transformers: an NLP Perspective](https://arxiv.org/abs/2311.17633)
- Arxiv - [An Introduction to Transformers](https://arxiv.org/abs/2304.10557)
- Arxiv - [Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2311.12351)
- Arxiv - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- Arxiv - [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)
- Medium - [Transformer alternatives in 2024](https://medium.com/nebius/transformer-alternatives-in-2024-06cd3d91d42b)  
- Huggingface - [Transformers-based Encoder-Decoder Models](https://huggingface.co/blog/encoder-decoder)
