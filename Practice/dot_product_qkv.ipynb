{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "透過實際復刻Scaled Dot-Product Attention來更熟悉注意力(Attention)機制\n",
    "\n",
    "<img height=\"300\" src=\"./Scaled_Dot-Product_Attention.png\" >  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個架構可以分成以下幾點：\n",
    "1. 定義QKV的來源，我們使用Word2Vec獲得\n",
    "2. MatMul (點積) 計算\n",
    "3. Scale (縮放)\n",
    "4. Mask (選擇性) 的用途\n",
    "5. SoftMax 的應用，使得注意力權重合為1\n",
    "6. MatMul with V 得到最終的輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 執行以下程式在新的Block\n",
    "```py\n",
    "%pip install gensim -q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 載入需要的函式庫以及pretrained的Word2Vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 下載並加載預訓練的Word2Vec模型\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個模型會將自然語言的詞轉成一個300維的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 完成以下ToDo part來完整整個Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行word embedding\n",
    "def word_embedding(word, model):\n",
    "    try:\n",
    "        return model[word]  # 嘗試從模型中獲取word embedding\n",
    "    except KeyError:\n",
    "        # 如果詞語不在模型的資料庫中，返回一個隨機向量\n",
    "        return np.random.randn(model.vector_size)  # 返回隨機向量，避免程式錯誤\n",
    "\n",
    "# SoftMax 函數\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # 減去最大值避免溢出\n",
    "    return np.exp(x - np.max(x)) / exp_x.sum(axis=-1, keepdims=True)  # 計算SoftMax並確保輸出形狀正確\n",
    "\n",
    "\n",
    "#########       TODO: 實現 Scaled Dot-Product Attention 函數      #########\n",
    "# Scaled Dot-Product Attention 函數\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \n",
    "    # 設定維度參數\n",
    "    # 縮放點積：計算Query和Key的內積，並除以維度的平方根\n",
    "    # 查看有沒有遮罩，如果有，將遮罩的位置設為負無窮\n",
    "    # 透過SoftMax函數計算注意力權重\n",
    "    # 計算MatMul with V的值\n",
    "    \n",
    "    return output, attention_weights # 返回輸出和注意力權重\n",
    "\n",
    "#########                        END TODO                        #########\n",
    "\n",
    "\n",
    "\n",
    "sentence_1 = \"the cat sat on the mat\"\n",
    "sentence_2 = \"dogs are playing in the park\"\n",
    "\n",
    "# 將句子分割為詞語列表\n",
    "words_1 = sentence_1.split()\n",
    "words_2 = sentence_2.split()\n",
    "\n",
    "# 將詞語轉換為embedding向量\n",
    "Q = np.array([word_embedding(word, model) for word in words_1])\n",
    "K = np.array([word_embedding(word, model) for word in words_2])\n",
    "V = np.array([word_embedding(word, model) for word in words_2])\n",
    "\n",
    "# 計算attention機制的輸出和權重\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# 視覺化attention權重\n",
    "def plot_attention_weights(attention_weights, sentence_1, sentence_2):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_weights, annot=True, cmap='coolwarm', xticklabels=sentence_2, yticklabels=sentence_1)  # 繪製熱圖並標註數值\n",
    "    plt.xlabel('Key (Sentence 2)')\n",
    "    plt.ylabel('Query (Sentence 1)')\n",
    "    plt.title('Attention Weights')\n",
    "    plt.show()\n",
    "\n",
    "plot_attention_weights(attention_weights, words_1, words_2)\n",
    "\n",
    "print(\"Output:\\n\", output)  # 輸出attention的最終結果\n",
    "print(\"Attention Weights:\\n\", attention_weights)  # 輸出attention權重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "結果應該會和以下的圖片一樣，或類似  \n",
    "<img height=\"500\" src=\"./output.png\" >  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解釋\n",
    "圖片中可以表示出\"the cat sat on the mat\" 和 \"dogs are playing in the park\"之間的注意力權重，其中：\n",
    "1. 權重越高，表示Query詞對該Key詞的關注度越高。\n",
    "2. 權重越低，表示Query詞對該Key詞的關注度越低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"cat\"對應於\"dogs\"的注意力權重為0.21，是所有權重中最高的。這表示模型認為\"cat\"在處理時應該特別關注\"dogs\"這個詞，可能因為它們都是動物，有較高的語義相關性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ToDo`完成後，可以將句子換成不同內容，更加了解注意力機制在處理句子時動態地為每個Query詞分配不同的權重這件事，例如\"cat\"對應於\"dogs\"的高權重，顯示出它們之間的高關聯性，代表模型認為在理解\"cat\"時，\"dogs\"是重要的上下文信息。\n",
    "\n",
    "這也是Transformer架構的核心概念，也是現在生成式模型可以透過自然語言處理的重要訓練架構設計。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
